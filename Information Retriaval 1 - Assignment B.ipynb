{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import scipy.stats\n",
    "import random\n",
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many results to look at\n",
    "AT = 3\n",
    "\n",
    "#Maximum relevance of a results\n",
    "MAXREL = 1\n",
    "\n",
    "#Bin labels include the first and exclude the last.\n",
    "BIN_LABELS = [[0.05, 0.1], [0.1,0.2], [0.2,0.3], [0.3,0.4], [0.4,0.5], [0.5,0.6], [0.6,0.7], [0.7, 0.8], [0.8,0.9], [0.9,0.95]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickSession:\n",
    "    #ClickSession is unique for the pair session id and query id\n",
    "    def __init__(self, _id, query_id, results):\n",
    "        self._id = _id\n",
    "        self.query_id = query_id\n",
    "        self.results = results #List of document ids\n",
    "        self.clicks_id = []\n",
    "        self.clicks_rank = [False for i in range(len(results))]\n",
    "\n",
    "    #The document_id maps to a result of self.results\n",
    "    def click(self, document_id):\n",
    "        self.clicks_rank[self.results.index(document_id)] = True\n",
    "        self.clicks_id.append(document_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log(filename):\n",
    "    sessions = []\n",
    "    with open(filename, 'r') as fp:\n",
    "        last_id = -1\n",
    "        for line in fp:\n",
    "            line = line.split()\n",
    "            if line[2] == \"Q\":\n",
    "                _id = line[0]\n",
    "                last_id = _id\n",
    "                query_id = line[3]\n",
    "                results = line[5:]\n",
    "                sessions.append(ClickSession(_id, query_id, results))\n",
    "            if line[2] == \"C\":\n",
    "                document_id = line[3]\n",
    "                i = -1\n",
    "                while sessions[i]._id == last_id and document_id not in sessions[i].results:\n",
    "                    i -= 1\n",
    "                sessions[i].click(document_id)\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A generic variable for the EM algorithm\n",
    "class EMX():\n",
    "    def __init__(self, sum=1., count=2.):\n",
    "        self.sum = sum\n",
    "        self.count = count\n",
    "\n",
    "    def value(self):\n",
    "        return min(self.sum / float(self.count), 0.999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClickModel(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def click(self, relevances):\n",
    "        pass\n",
    "\n",
    "class RCM(ClickModel):\n",
    "    \n",
    "    def __init__(self, max_at=3):\n",
    "        self.max_at=3\n",
    "        self.p = 0.5\n",
    "        self.estimate_parameters(parseYandexLog(\"./YandexRelPredChallenge.txt\"))\n",
    "    \n",
    "    def click(self, relevances):\n",
    "        return np.random.binomial(1, self.p, size=len(relevances))\n",
    "\n",
    "    def estimate_parameters(self, sessions, at=10):\n",
    "        n_shown_docs = 0\n",
    "        n_clicks = 0\n",
    "        for s in sessions:\n",
    "            n_shown_docs += len(s.clicks_rank)\n",
    "            n_clicks += sum(s.clicks_rank)\n",
    "        self.p = n_clicks / n_shown_docs\n",
    "\n",
    "class PBM(ClickModel):\n",
    "    def __init__(self, at=AT, p_attraction=0.95, file_p_examinations=None):\n",
    "        self.p_attraction = p_attraction\n",
    "        self.p_examination = np.random.random(at)\n",
    "        if file_p_examinations != None:\n",
    "            with open(file_p_examinations, \"r\") as f:\n",
    "                self.p_examination = json.load(f)\n",
    "\n",
    "    def click_probabilities(self, relevances):\n",
    "        p_attraction = np.array([self.p_attraction if relevance == 1 else 1-self.p_attraction for relevance in relevances])\n",
    "        cDist = p_attraction * self.p_examination[:len(relevances)]\n",
    "        return cDist\n",
    "\n",
    "    def simulate_first_click(self, relevances):\n",
    "        cDist = self.click_probabilities(relevances)\n",
    "        cDist /= cDist.sum()\n",
    "        chosen_ranking = np.random.choice(len(relevances), p = cDist)\n",
    "        return chosen_ranking\n",
    "\n",
    "    #Returns list of boleeans stating if a document was clicked\n",
    "    def click(self, relevances):\n",
    "        cDist = self.click_probabilities(relevances)\n",
    "        return np.random.binomial(1, cDist, size=len(relevances))\n",
    "\n",
    "    #initial_attraction/initial_examination is a function that returns the initial attraction of a query_id, document_id pair\n",
    "    def estimate(self, sessions, at=10, verbose=False):\n",
    "        p_examination = [EMX()  for i in range(at)]\n",
    "        p_attractiveness = {} #Dictionary that maps from (query_id, document_id) to [[sum, count], old_attractiveness]\n",
    "        old_examination = np.zeros(at)\n",
    "        tol = 0.001\n",
    "        while np.mean([abs(p_examination[i].value() - old_examination[i]) for i in range(at)]) > tol:\n",
    "            old_examination = [p.value() for p in p_examination]\n",
    "            for s in sessions:\n",
    "                query_id = s.query_id\n",
    "                for rank, document_id in enumerate(s.results):\n",
    "                    attractiveness = p_attractiveness.get((query_id, document_id), EMX())\n",
    "\n",
    "                    old_attr = attractiveness.value()\n",
    "                    old_exam = p_examination[rank].value()\n",
    "\n",
    "                    attractiveness.count += 1.\n",
    "                    p_examination[rank].count += 1.\n",
    "\n",
    "                    if s.clicks_rank[rank]:\n",
    "                        attractiveness.sum += 1.\n",
    "                        p_examination[rank].sum += 1.\n",
    "                    else:\n",
    "                        attractiveness.sum      += (1.-old_exam) * old_attr / (1. - old_attr * old_exam)\n",
    "                        p_examination[rank].sum += (1.-old_attr) * old_exam / (1. - old_attr * old_exam)\n",
    "                    p_attractiveness[(query_id, document_id)] = attractiveness\n",
    "\n",
    "            if verbose:\n",
    "              print(\"EXAMINATION\")\n",
    "              for rank in range(at):\n",
    "                  print('{:.3e}   {:.3e} {:.3e}'.format(p_examination[rank].sum, p_examination[rank].count, p_examination[rank].value()))\n",
    "\n",
    "              print(\"ATTRACTION\")\n",
    "              i=0\n",
    "              for key, value in p_attractiveness.items():\n",
    "                  if i<1000 and i%100 == 5:\n",
    "                      print('{:.3e}   {:.3e} {:.3e}'.format(value.sum, value.count, value.value()))\n",
    "                  i+=1\n",
    "              print(\"\\n\\n\\n\")\n",
    "        return [p.value() for p in p_examination]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_combinations(relevances, at=0, max_rel=MAXREL):\n",
    "    for rel in range(max_rel + 1):\n",
    "        relevances[at] = rel\n",
    "        if at + 1 == len(relevances):\n",
    "            yield relevances\n",
    "        else:\n",
    "            yield from relevance_combinations(relevances, at+1)\n",
    "\n",
    "def generate_relevance_pairs(at=AT):\n",
    "    for rel_left in relevance_combinations(np.zeros(at)):\n",
    "        for rel_right in relevance_combinations(np.zeros(at)):\n",
    "            yield (np.array(rel_left), np.array(rel_right))\n",
    "\n",
    "def calculate_err(relevances, at=AT):\n",
    "    thetas = (np.power(2, relevance) - 1) / 2**max_rel\n",
    "    sum_res = 0\n",
    "    for r in range(at):\n",
    "        prod = 1\n",
    "        for i in range(r):\n",
    "            prod *= (1-thetas[i])\n",
    "        prod *= thetas[r]\n",
    "        sum_res += prod * (1/(r+1))\n",
    "    return sum_res\n",
    "\n",
    "def find_bin_index(delta_err):\n",
    "    for i, bini in enumerate(BIN_LABELS):\n",
    "        if delta_err >= bini[0] and delta_err < bini[1]:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def generate_document_overlaps(rel_pair, same_as=([None, None, None], [None, None, None]), at=0):\n",
    "    if at == len(rel_pair[0]):\n",
    "        yield same_as\n",
    "        return\n",
    "    yield from generate_document_overlaps(rel_pair, same_as=same_as, at=at+1)\n",
    "    for idx, rel_r in enumerate(rel_pair[1]):\n",
    "        if rel_pair[0][at] == rel_r and idx not in same_as[0]:\n",
    "            same_as[0][at] = idx\n",
    "            same_as[1][idx] = at\n",
    "            yield from generate_document_overlaps(rel_pair, same_as=same_as, at=at+1)\n",
    "            same_as[0][at] = None\n",
    "            same_as[1][idx] = None\n",
    "\n",
    "def interleave(rel_pair, method, k=2000, at=AT):\n",
    "    softmax = lambda at: 1/(np.arange(at)+1)**3\n",
    "    if method == 'teamdraft':\n",
    "        initial_dist = np.ones((2,at),bool)\n",
    "        chooser = lambda probDistr, r: [probDistr[r].argmax()], [r]\n",
    "    elif method == 'probabilistic':\n",
    "        initial_dist = np.array([softmax(at), softmax(at)])\n",
    "        chooser = lambda probDistr, r: ([np.random.choice(at, p=probDistr[r]/probDistr[r].sum()),\n",
    "                                         np.random.choice(at, p=probDistr[r]/probDistr[r].sum())],\n",
    "                                        [r, not r])\n",
    "    for same_as, _ in product(generate_document_overlaps(rel_pair), range(k)):\n",
    "        probDistr = initial_dist.copy()\n",
    "        relevances, attribution = np.zeros(at), np.zeros(at)\n",
    "        y = 0\n",
    "        while y < at:\n",
    "            R = random.randint(0,1)\n",
    "            if probDistr[R].sum() == 0:\n",
    "                R = not R\n",
    "            idxs, attr = chooser(probDistr, R)\n",
    "            for _y in range(min(at - y, len(idxs))):\n",
    "                idx, r = idxs[_y], attr[_y]\n",
    "                probDistr[r][idx] = 0\n",
    "                if same_as[r][idx]:\n",
    "                    probDistr[not r][same_as[r][idx]] = 0\n",
    "                relevances[y] = rel_pair[r][idx]\n",
    "                attribution[y] = r\n",
    "                y += 1\n",
    "        yield relevances, attribution\n",
    "\n",
    "def generate_DERR_bins():\n",
    "    filled_bins = [[] for _ in range(len(BIN_LABELS))] # list of [(relevance_E, relevance_P), ...]. Each list corresponds to the bin with limits in bin_labels\n",
    "    for relevances_E, relevances_P in generate_relevance_pairs():\n",
    "        err_E = calculate_err(relevances_E)\n",
    "        err_P = calculate_err(relevances_P)\n",
    "        delta_err = err_E - err_P\n",
    "        bIdx = find_bin_index(delta_err)\n",
    "        if bIdx != -1:\n",
    "            filled_bins[bIdx].append((relevances_E, relevances_P))\n",
    "    return filled_bins\n",
    "\n",
    "def simulate_online_experiment(rel_pair, click_model, N=70, interleaving_method='teamdraft'):\n",
    "    victories = [0,0]\n",
    "    for relevances, attribution in interleave(rel_pair, method=interleaving_method):\n",
    "        #clicks are a list of booleans that represent whether that rank was clicked or not\n",
    "        pair_wins_E = 0\n",
    "        pair_wins_P = 0\n",
    "        for _ in range(N):\n",
    "            clicks = click_model.click(relevances) #\n",
    "            clicksE = sum(click for i, click in enumerate(clicks) if attribution[i]==0)\n",
    "            clicksP = sum(click for i, click in enumerate(clicks) if attribution[i]==1)\n",
    "            if clicksE > clicksP:\n",
    "                victories[0] += 1\n",
    "                pair_wins_E += 1\n",
    "            elif clicksP > clicksE:\n",
    "                victories[1] += 1\n",
    "                pair_wins_P += 1\n",
    "    return victories[0]/(victories[0] + victories[1])\n",
    "\n",
    "def estimate_sample_size(p1, alpha=0.05, beta=0.90, p0=0.5):\n",
    "    z_alpha = scipy.stats.norm.ppf(1-alpha)\n",
    "    z_beta = scipy.stats.norm.ppf(beta)\n",
    "    nDash = ((z_alpha * math.sqrt(p0 * (1-p0)) + z_beta * math.sqrt(p1 * (1-p1))) / abs(p1 - p0))**2\n",
    "    n = nDash + (1 / abs(p1 - p0))\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = parse_log(\"./YandexRelPredChallenge.txt\")\n",
    "print('PARSED YANDEX CLICK LOG')\n",
    "Pbm = PBM(3)\n",
    "prob_examination = Pbm.estimate(sessions)\n",
    "with open('p_examinations.json', 'w') as fp:\n",
    "    json.dump(prob_examination, fp)\n",
    "print(prob_examination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(click_model, interleaving):\n",
    "  bins = generate_DERR_bins()\n",
    "  for bIdx, _bin in enumerate(bins):\n",
    "      if len(_bin) == 0:\n",
    "          print('Empty bin: {}'.format(BIN_LABELS[bIdx]))\n",
    "          continue\n",
    "      Ns = []\n",
    "      pVictorys = []\n",
    "      for rel_pair in _bin:\n",
    "          pVictory = simulate_online_experiment(rel_pair, pbm, interleaving_method='teamdraft')\n",
    "          n = estimate_sample_size(pVictory)\n",
    "          Ns.append(n)\n",
    "          pVictorys.append(pVictory)\n",
    "      Ns = np.array(Ns)\n",
    "      pVictorys = np.array(pVictorys)\n",
    "      print('{}\\t\\tMedianPWins: {: 3.2f}, Min: {: 3.2f}, Median: {: 7.2f}, Max: {: 7.2f}'.format(\n",
    "          BIN_LABELS[bIdx],\n",
    "          pVictorys.median(),\n",
    "          Ns.min(),\n",
    "          np.median(Ns),\n",
    "          Ns.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbm = PBM(file_p_examinations=\"p_examinations.json\")\n",
    "rcm = RCM()\n",
    "\n",
    "run_experiment(rcm, 'teamdraft')\n",
    "run_experiment(rcm, 'probabilistic')\n",
    "run_experiment(pbm, 'teamdraft')\n",
    "run_experiment(pbm, 'probabilistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Click Model \n",
    "\n",
    "### TeamDraft Interleaving\n",
    "| $\\Delta$-ERR | Min | Median | Max |\n",
    "| --- | --- | --- | --- |\n",
    "| [0.05, 0.1) | 518999 | 3064617 | 7759520 |\n",
    "| [0.1, 0.2) | 126857 | 393734 | 6411169 |\n",
    "| [0.2, 0.3) | 1434330 | 15838539 | 32170237 |\n",
    "| [0.3, 0.4) | 47816 | 198487 | 2515511 |\n",
    "| [0.4, 0.5) | 376168 | 499254 | 136808724 |\n",
    "| [0.5, 0.6) | 604695 | 841991 | 15336869 |\n",
    "| [0.6, 0.7) | 54409 | 77735 | 101060 |\n",
    "| [0.7, 0.8) | - | - | - |\n",
    "| [0.8, 0.9) | - | - | - |\n",
    "| [0.9, 0.95] | - | - | - |\n",
    "\n",
    "### Probabilistic Interleaving\n",
    "| $\\Delta$-ERR | Min | Median | Max |\n",
    "| --- | --- | --- | --- |\n",
    "|[0.05, 0.1) | 139130 | 317076 | 13086712 |\n",
    "|[0.1, 0.2) | 82458 | 444991 | 1969155 |\n",
    "|[0.2, 0.3) | 1567851 | 6167637 | 18540452 |\n",
    "|[0.3, 0.4) | 152076 | 859388 | 2223242 |\n",
    "|[0.4, 0.5) | 179671 | 643490 | 5018302 |\n",
    "|[0.5, 0.6) | 528226 | 625405 | 944732 |\n",
    "|[0.6, 0.7) | 29994 | 45809 | 61624 |\n",
    "|[0.7, 0.8) | - | - | - |\n",
    "|[0.8, 0.9) | - | - | - |\n",
    "|[0.9, 0.95] | - | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Based Model \n",
    "\n",
    "### TeamDraft Interleaving\n",
    "| $\\Delta$-ERR | Min | Median | Max |\n",
    "| --- | --- | --- | --- |\n",
    "| [0.05, 0.1) | 34 | 3439 | 465921 |\n",
    "| [0.1, 0.2) | 31 | 81 | 265 |\n",
    "| [0.2, 0.3) | 15 | 29 | 44 |\n",
    "| [0.3, 0.4) | 11 | 26 | 29 |\n",
    "| [0.4, 0.5) | 11 | 11 | 24 |\n",
    "| [0.5, 0.6) | 7 | 7 | 11 |\n",
    "| [0.6, 0.7) | 7 | 7 | 7 |\n",
    "| [0.7, 0.8) | - | - | - |\n",
    "| [0.8, 0.9) | - | - | - |\n",
    "| [0.9, 0.95] | - | - | - |\n",
    "\n",
    "\n",
    "### Probabilistic Interleaving\n",
    "| $\\Delta$-ERR | Min | Median | Max |\n",
    "| --- | --- | --- | --- |\n",
    "| [0.05, 0.1) | 94 | 378 | 1771 |\n",
    "| [0.1, 0.2) | 31 | 62 | 235 |\n",
    "| [0.2, 0.3) | 14 | 38 | 54 |\n",
    "| [0.3, 0.4) | 11 | 25 | 35 |\n",
    "| [0.4, 0.5) | 14 | 15 | 23 |\n",
    "| [0.5, 0.6) | 7 | 7 | 13 |\n",
    "| [0.6, 0.7) | 7 | 7 | 7 |\n",
    "| [0.7, 0.8) | - | - | - |\n",
    "| [0.8, 0.9) | - | - | - |\n",
    "| [0.9, 0.95] | - | - | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment design options\n",
    "\n",
    "- During the inference of the position based model’s parameters we infer 10 probabilities of examination, one for each rank that appears in the Yandex Click Log against only taking into consideration the first 3 results of each query. After this we use only the first 3. This choice was made to get the most accurate values of the probabilities of attraction, by considering clicks outside of the first 3 and therefore the most accurate examination probabilities.\n",
    "- During the experiment we generate all possible pairs of relevances that fit the bins and, when interleaving, all possibilities regarding equality of documents. This gives the same probability to all the cases which may be quite different from the real distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "First of all we can see that a the random click model, as expected, has a very high number of minimum samples to be statistical significant which is expected as the proportion of wins of ranker E should be and is the same as the proportion of wins of ranker P.\n",
    "Regarding the position based model, it’s interesting to note that in both interleaving methods the median and the maximum minimum number of samples needed are at least a order of magnitude greater when the difference between ERRs are between 0.05 and 0.1. This is due to the proportion of wins in most of the relevances pairs being closer to 0.5 which, creates a exponential growth in the number of samples needed to have statistical significance.\n",
    "We can also see that the median number of impressions required in this first bin is about ten times larger with the team draft interleaving compared with the probabilistic interleaving BECAUSE (STATE ADVANTAGES OF PROBABILISTIC INTERLEAVING).\n",
    "Homework Assignment - Part B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
